{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json \n",
    "import os\n",
    "import io\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from pprint import pprint\n",
    "from shutil import copyfile, move\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = .7\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "as_grey = False\n",
    "batch_size = 16\n",
    "test_batch_size = 16\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# junk model for display in presentation\n",
    "n_model = keras.models.load_model(os.getcwd() + '/models/cloud_July 04, 2018.hdf5')\n",
    "# n_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best non pretrained resnet for indiviuals\n",
    "# n_model = keras.models.load_model(os.getcwd() + '/models/cloud_custom_August 19, 2018.hdf5')\n",
    "\n",
    "# best pretrained resnet for indiviuals\n",
    "# n_model = keras.models.load_model(os.getcwd() + '/models/cloud_pretrained_August 19, 2018.hdf5')\n",
    "\n",
    "# adapted species resnet for indiviuals\n",
    "# n_model = keras.models.load_model('D:/Species_Snapshots/Tuesday_10_2_2018_Individual_freeze/weights-improvement-184-0.58.hdf5')\n",
    "# second adapted resnet worse\n",
    "# n_model2 = keras.models.load_model('D:/Species_Snapshots/Tuesday_10_2_2018_Individual_freeze/weights-improvement-416-0.96.hdf5')`\n",
    "\n",
    "# best pretrained resnet for species\n",
    "n_model = keras.models.load_model('D:/Species_Snapshots/Wednesday_9_19_2018_inital/weights-improvement-24-0.95.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102288 images belonging to 87 classes.\n"
     ]
    }
   ],
   "source": [
    "from vis.visualization import visualize_cam\n",
    "from vis.utils import utils\n",
    "\n",
    "# test_path = os.getcwd() + '/data/Clouded_leopard_ID/test_cleaned_cropped'\n",
    "test_path = 'D:/Species/test_cropped'\n",
    "# test_path = 'U:/PycharmProjects/AnimalBiometrics/data/Clouded_leopard_ID/Individual/test_cleaned'\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "data_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "data_list = []\n",
    "batch_index = 0\n",
    "\n",
    "# while batch_index <= data_generator.batch_index:\n",
    "#     data = data_generator.next()\n",
    "#     data_list.append(data[0][0])\n",
    "#     batch_index += 1\n",
    "    \n",
    "# data_array = np.asarray(data_list)\n",
    "# data_array_label = np.asarray(data_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {v: k for k, v in data_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n = 102288\n",
    "# for _ in range(n):\n",
    "#     data = data_generator.next()\n",
    "#     data_list.append(data[0][0])\n",
    "    \n",
    "# data_array = np.asarray(data_list)\n",
    "# # data_array_label = np.asarray(data_generator.classes[:data_generator.batch_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load existing prediction\n",
    "prediction_heat = pd.read_csv('./Prediction_Species_all.csv', usecols=[1]).values.flatten()\n",
    "data_array_label = data_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 8s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "# make new prediction\n",
    "prediction_heat_all = n_model.predict_generator(data_generator, verbose=1)\n",
    "prediction_heat = np.argmax(prediction_heat_all,axis=-1)\n",
    "data_array_label = data_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "pd.DataFrame(np.concatenate([np.array(data_generator.filenames)[:,np.newaxis], prediction_heat[:,np.newaxis],data_array_label[:,np.newaxis]],axis=1), columns=['Image_path','Prediction', 'True']).to_csv('Prediction_Species_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 48s 423ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_all = n_model.evaluate_generator(data_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = (prediction_heat == data_array_label).nonzero()[0]\n",
    "wrong_pred = (prediction_heat != data_array_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9488111997497263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data_array_label, prediction_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "from vis.visualization import visualize_saliency\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def compute_saliency_map(model, array, target_class, layer_idx=-1):\n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=target_class, seed_input=array)\n",
    "#     print(grads.shape)\n",
    "    smoothe = ndimage.gaussian_filter(grads, sigma=5) \n",
    "    return smoothe\n",
    "\n",
    "def render_img_on_grid(img, pos, grid):\n",
    "    ax = plt.subplot(grid[pos])\n",
    "    ax.imshow(img)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "def show_side_by_side(img, saliency_map, pred, true, i=None, save_path='C:\\\\Users\\\\fotto\\\\Dropbox'):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    grid = gridspec.GridSpec(1, 2, wspace=0.)\n",
    "    render_img_on_grid(img, 0, grid)\n",
    "    text = 'True: ' + true\n",
    "    plt.title(text, size=\"xx-large\")\n",
    "    ax = render_img_on_grid(img, 1, grid)\n",
    "    ax.imshow(saliency_map, alpha=.7)\n",
    "    text = 'Pred: ' + pred\n",
    "    ax.set_title(text, size=\"xx-large\")\n",
    "    if i is not None and save_path:\n",
    "        root = save_path + '{}_{}_1'.format(true, pred)\n",
    "        file_name = 'Activation_' + str(i) + '.jpg'\n",
    "        path = os.path.join(root, file_name)\n",
    "\n",
    "        if os.path.isdir(root):\n",
    "            while os.path.isfile(path):\n",
    "                root = root[:-1] + str(int(root[-1])+1)\n",
    "                path = os.path.join(root, file_name)\n",
    "        if not os.path.isdir(root):\n",
    "            os.mkdir(root)\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_v2(indices, images, predictions, labels, trained_model, activation_layer='predictions'):\n",
    "    \n",
    "#     trained_model.layers[utils.find_layer_idx(trained_model, \"predictions\")].activation = keras.activations.linear\n",
    "    \n",
    "    for i, img_number in enumerate(indices):\n",
    "        saliency_map = compute_saliency_map(trained_model, array=images[img_number],\n",
    "                                            target_class=[labels[img_number]], layer_idx=utils.find_layer_idx(trained_model, activation_layer))\n",
    "        show_side_by_side(images[img_number], saliency_map, class_dict[predictions[img_number]], class_dict[labels[img_number]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_comparison_v2(wrong_pred, data_array, prediction_heat, data_array_label, n_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_comparison_v2(correct_pred, data_array, prediction_heat, data_array_label, n_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dist(img_path=\"U:\\\\PycharmProjects\\\\AnimalBiometrics\\\\data\\\\Clouded_leopard_ID\\\\\"):\n",
    "    res = {}\n",
    "    for subdir, dirs, files in os.walk(img_path):\n",
    "#         if 'coarse' not in subdir.lower():\n",
    "#             continue\n",
    "        if dirs == []:\n",
    "            idx = subdir.rfind(\"\\\\\")+1\n",
    "            res[subdir[idx:]]=len(files)    \n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_res = get_data_dist(img_path=\"D:\\\\Species\\\\train_cropped\")\n",
    "# dist_res = get_data_dist('U:/PycharmProjects/AnimalBiometrics/data/Clouded_leopard_ID/Individual/train_cleaned_copy')\n",
    "res = collections.OrderedDict(sorted(dist_res.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = (prediction_heat == data_generator.classes).nonzero()[0]\n",
    "wrong_pred = (prediction_heat != data_generator.classes).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_per_class(n, data_generator, model, predictions, correct_idx, wrong_idx, res_dict, class_dict, layers=[]):\n",
    "\n",
    "    for animal in list(res_dict.keys()):\n",
    "        names = np.array(data_generator.filenames)\n",
    "        idx = np.flatnonzero(np.core.defchararray.find(names,animal)!=-1)\n",
    "        idx_correct = np.intersect1d(idx, correct_idx)\n",
    "        idx_wrong = np.intersect1d(idx, wrong_idx)\n",
    "\n",
    "        if len(idx_correct) > n:\n",
    "            idx_correct = np.random.choice(idx_correct, n)\n",
    "\n",
    "        if len(idx_wrong) > n:\n",
    "            idx_wrong = np.random.choice(idx_wrong, n)\n",
    "\n",
    "#         if len(idx) > n:\n",
    "#             idx = np.random.choice(idx, n)\n",
    "        \n",
    "#         print('Examples for {}'.format(animal))\n",
    "#         for elem in idx:\n",
    "#             data_generator.batch_index = elem\n",
    "#             image = data_generator.next()[0][0]\n",
    "#             prediction = np.argmax(model.predict(np.expand_dims(image, axis=0)))\n",
    "#             target_class = data_generator.classes[elem]\n",
    "            \n",
    "#             saliency_map = compute_saliency_map(model, array=image, target_class=[target_class],\n",
    "# #                                                 layer_idx=utils.find_layer_idx(model, 'predictions')\n",
    "#                                                )\n",
    "\n",
    "#             show_side_by_side(image, saliency_map, class_dict[prediction], class_dict[target_class])\n",
    "\n",
    "        print('Wrong cases for {}'.format(animal))\n",
    "        for elem in idx_wrong:\n",
    "            data_generator.batch_index = elem\n",
    "            image = data_generator.next()[0][0]\n",
    "            target_class = data_generator.classes[elem]\n",
    "            \n",
    "            if not layers:\n",
    "                saliency_map = compute_saliency_map(model, array=image, target_class=[target_class])\n",
    "                show_side_by_side(image, saliency_map, class_dict[predictions[elem]], class_dict[target_class])\n",
    "            else:\n",
    "                for i in layers:\n",
    "                    saliency_map = compute_saliency_map(model, array=image, target_class=[target_class],\n",
    "                                                    layer_idx=utils.find_layer_idx(model, 'activation_{}'.format(i))\n",
    "                                                   )\n",
    "                    show_side_by_side(image, saliency_map, class_dict[predictions[elem]], class_dict[target_class], i)\n",
    "\n",
    "        print('Correct cases for {}'.format(animal))\n",
    "        for elem in idx_correct:\n",
    "            data_generator.batch_index = elem\n",
    "            image = data_generator.next()[0][0]\n",
    "            target_class = data_generator.classes[elem]\n",
    "            \n",
    "            if not layers:\n",
    "                saliency_map = compute_saliency_map(model, array=image, target_class=[target_class])\n",
    "                show_side_by_side(image, saliency_map, class_dict[predictions[elem]], class_dict[target_class])\n",
    "            else:\n",
    "                for i in layers:\n",
    "                    saliency_map = compute_saliency_map(model, array=image, target_class=[target_class],\n",
    "                                                        layer_idx=utils.find_layer_idx(model, 'activation_{}'.format(i))\n",
    "                                                       )\n",
    "                    show_side_by_side(image, saliency_map, class_dict[predictions[elem]], class_dict[target_class], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong cases for Asian small-clawed otter\n",
      "Correct cases for Asian small-clawed otter\n",
      "Wrong cases for Banded civet\n",
      "Correct cases for Banded civet\n",
      "Wrong cases for Banded linsang\n",
      "Correct cases for Banded linsang\n",
      "Wrong cases for Banteng\n",
      "Correct cases for Banteng\n",
      "Wrong cases for Bay cat\n",
      "Correct cases for Bay cat\n",
      "Wrong cases for Bearded pig\n",
      "Correct cases for Bearded pig\n",
      "Wrong cases for Binturong\n",
      "Correct cases for Binturong\n",
      "Wrong cases for Bird\n",
      "Correct cases for Bird\n",
      "Wrong cases for Black-capped babbler\n"
     ]
    }
   ],
   "source": [
    "data_generator.batch_index = 1\n",
    "compare_per_class(2, data_generator, n_model, prediction_heat, correct_pred, wrong_pred res, class_dict, layers=range(50, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_labels(true, pred, count_dict):\n",
    "    true_copy = np.copy(true)\n",
    "    pred_copy = np.copy(pred)\n",
    "    class_dict= {}\n",
    "        \n",
    "    for animal, count in count_dict.items():\n",
    "            \n",
    "        label_idx = (true_copy == animal).nonzero()[0]\n",
    "        pred_idx = (pred_copy == animal).nonzero()[0]\n",
    "\n",
    "        if animal in ['Team', 'Ghost', 'Unknown', 'Human', 'Poacher', 'SFD Staff', 'Unsure', 'Vehicle', 'Encroacher']:\n",
    "            top_level = 'No Animal'\n",
    "            key = 0\n",
    "        elif count >= 1000:\n",
    "            top_level = 'Normal > 1000'\n",
    "            key = 1\n",
    "        elif count >= 500:\n",
    "            top_level = 'Scarce > 500'\n",
    "            key = 2\n",
    "        elif count >= 200:\n",
    "            top_level = 'Rare > 200'\n",
    "            key = 3\n",
    "        elif count >= 50:\n",
    "            top_level = 'Very Rare > 50'\n",
    "            key = 4\n",
    "        elif count < 50:\n",
    "            top_level = 'Extremely Rare < 50'\n",
    "            key = 5\n",
    "            \n",
    "        if key not in class_dict.keys():\n",
    "            class_dict[key] = top_level\n",
    "        \n",
    "        true_copy[label_idx] = top_level\n",
    "        pred_copy[pred_idx] = top_level\n",
    "    return true_copy, pred_copy, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          class_dict,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          selection=None,\n",
    "                          font_size=30):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm[np.isnan(cm)] = 0.\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig = plt.figure(1, figsize=(15,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=font_size)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "    ordered_classes = collections.OrderedDict(sorted(class_dict.items()))\n",
    "    tick_marks = np.arange(len(ordered_classes.keys()))\n",
    "#     ticks = [elem if not elem == 'Extremely Rare < 50' else 'Extremely\\n Rare < 50' for elem in ordered_classes.values()]\n",
    "    plt.yticks(tick_marks, ticks, fontsize=font_size)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=font_size-5)\n",
    "\n",
    "    plt.ylabel('True label', size=\"large\")\n",
    "#     plt.xlabel('Predicted label', size=\"large\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if selection is not None:\n",
    "        plt.figure(2, figsize=(20,15))\n",
    "        for i, elem in enumerate(ordered_classes.values()):\n",
    "            ax = plt.subplot(1,len(ordered_classes.values()),i+1, frameon=False)\n",
    "            ax.imshow(selection[elem],interpolation='nearest')\n",
    "            ax.axis('off')\n",
    "    else: \n",
    "        plt.xticks(tick_marks, ordered_classes.values(), rotation=45, fontsize=font_size)\n",
    "        \n",
    "    plt.figure(1)\n",
    "    plt.savefig('conf_mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_array_names = [class_dict[elem] for elem in data_array_label]\n",
    "prediction_heat_names = [class_dict[elem] for elem in prediction_heat]\n",
    "\n",
    "cnf_matrix = confusion_matrix(data_array_names, prediction_heat_names, labels=list(class_dict.values()))\n",
    "cnf_matrix[np.isnan(cnf_matrix)] = 0\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, class_dict=class_dict, title='Confusion matrix',normalize=True)\n",
    "print(classification_report(data_array_names, prediction_heat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_array_names = [class_dict[elem] for elem in data_array_label]\n",
    "prediction_heat_names = [class_dict[elem] for elem in prediction_heat]\n",
    "\n",
    "cnf_matrix = confusion_matrix(data_array_names, prediction_heat_names, labels=list(class_dict.values()))\n",
    "cnf_matrix[np.isnan(cnf_matrix)] = 0\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, class_dict=class_dict, title='Confusion matrix',normalize=True)\n",
    "print(classification_report(data_array_names, prediction_heat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 7, 2: 7, 3: 10, 4: 5})\n"
     ]
    }
   ],
   "source": [
    "data_array_names_simple, prediction_heat_names_simple, class_dict_simple = simplify_labels(data_array_names, prediction_heat_names, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of image to select for each class\n",
    "selection = {\n",
    "    'Extremely Rare < 50': 102, \n",
    "    'No Animal' : 10, \n",
    "    'Normal > 1000': 30, \n",
    "    'Rare > 200': 1,\n",
    "    'Scarce > 500': 10, \n",
    "    'Very Rare > 50': 10\n",
    "}\n",
    "\n",
    "def get_img_from_selection(selection=selection):\n",
    "    image_selection = {}\n",
    "    for i, cat in enumerate(np.unique(data_array_names_simple)):\n",
    "        idx_animal = (data_array_names_simple == cat).nonzero()[0]\n",
    "        data_generator.batch_index = idx_animal[selection[cat]]\n",
    "        image = data_generator.next()[0][0]\n",
    "    #     plt.title(cat + \"    \" + str(idx_animal[selection[cat]]))\n",
    "        image_selection[cat] = image\n",
    "    #     plt.imshow(image)\n",
    "    #     plt.show()\n",
    "    #     plt.close()\n",
    "    return image_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(data_array_names_simple, prediction_heat_names_simple, labels=list(class_dict_simple.values()))\n",
    "cnf_matrix[np.isnan(cnf_matrix)] = 0\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, class_dict=class_dict_simple, title='Confusion matrix',\n",
    "                      normalize=True, selection=None)\n",
    "print(classification_report(data_array_names_simple, prediction_heat_names_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 408979 images belonging to 87 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'D://Species//train_cropped'\n",
    "\n",
    "data_generator_train = test_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "train_names = [class_dict[elem] for elem in data_generator_train.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names_simple, _, _ = simplify_labels(train_names, train_names, get_data_dist(train_path))\n",
    "res_train = np.unique(train_names_simple, return_counts=True)\n",
    "res_train = dict(zip(res_train[0],res_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_res = collections.OrderedDict(sorted(res_train.items()))\n",
    "\n",
    "size = len(sorted_res.keys())\n",
    "\n",
    "plt.figure(0, figsize=(15,20))\n",
    "plt.barh(list(sorted_res.keys()), list(sorted_res.values()))\n",
    "plt.xticks(rotation='vertical', size='xx-large')\n",
    "# plt.vlines(200, -1, size + 1, label='split at 200')\n",
    "# plt.vlines(100, -1, size + 1 , label='split at 100')\n",
    "# plt.vlines(50, -1, size + 1, label='split at 50')\n",
    "# plt.ylim(ymax=1000)\n",
    "# plt.legend()\n",
    "\n",
    "sorted_res_value = collections.OrderedDict(sorted(res_train.items(), key=lambda kv: kv[1]))\n",
    "\n",
    "plt.figure(1, figsize=(15,20))\n",
    "plt.barh(list(sorted_res_value.keys()), list(sorted_res_value.values()))\n",
    "plt.xticks(rotation='vertical', fontsize=30)\n",
    "plt.yticks(list(sorted_res_value.keys()), fontsize=30)\n",
    "# plt.xlim(xmax=1000)\n",
    "# plt.vlines(200, -1, size + 1, label='split at 200')\n",
    "# plt.vlines(100, -1, size + 1, label='split at 100')\n",
    "# plt.vlines(50, -1, size + 1, label='split at 50')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(path, split=.2):\n",
    "        for subdir, dirs, files in os.walk(path):\n",
    "            if dirs == []:\n",
    "                test_size = math.ceil(len(files) * split)\n",
    "                idx = np.array(random.sample(range(0, len(files)), test_size))\n",
    "                files = np.array(files)\n",
    "                subdir_new = str(subdir).replace('\\\\cropped\\\\','\\\\test_cropped\\\\')\n",
    "                print(subdir_new)\n",
    "                if not (os.path.exists(subdir_new) and os.path.isdir(subdir_new)):\n",
    "                    os.makedirs(subdir_new)\n",
    "                for elem in files[idx]:\n",
    "                    file = subdir + '\\\\' + elem\n",
    "                    move(file, subdir_new + '\\\\' + elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Species\\test_cropped\\Asian small-clawed otter\n",
      "D:\\Species\\test_cropped\\Banded civet\n",
      "D:\\Species\\test_cropped\\Banded linsang\n",
      "D:\\Species\\test_cropped\\Banteng\n",
      "D:\\Species\\test_cropped\\Bay cat\n",
      "D:\\Species\\test_cropped\\Bearded pig\n",
      "D:\\Species\\test_cropped\\Binturong\n",
      "D:\\Species\\test_cropped\\Bird\n",
      "D:\\Species\\test_cropped\\Black-capped babbler\n",
      "D:\\Species\\test_cropped\\Black-crowned pitta\n",
      "D:\\Species\\test_cropped\\Blue flycather\n",
      "D:\\Species\\test_cropped\\Blue-headed pitta\n",
      "D:\\Species\\test_cropped\\Bornean ground-cuckoo\n",
      "D:\\Species\\test_cropped\\Bornean necklaced partridge\n",
      "D:\\Species\\test_cropped\\Bornean peacock pheasant\n",
      "D:\\Species\\test_cropped\\Bornean yellow muntjac\n",
      "D:\\Species\\test_cropped\\Buff-vented bulbul\n",
      "D:\\Species\\test_cropped\\Bushy fish owl\n",
      "D:\\Species\\test_cropped\\Changeable hawk eagle\n",
      "D:\\Species\\test_cropped\\Clouded leopard\n",
      "D:\\Species\\test_cropped\\Collared mongoose\n",
      "D:\\Species\\test_cropped\\Common palm civet\n",
      "D:\\Species\\test_cropped\\Common porcupine\n",
      "D:\\Species\\test_cropped\\Crested fireback\n",
      "D:\\Species\\test_cropped\\Crested serpent eagle\n",
      "D:\\Species\\test_cropped\\Dog\n",
      "D:\\Species\\test_cropped\\Elephant\n",
      "D:\\Species\\test_cropped\\Emerald dove\n",
      "D:\\Species\\test_cropped\\Encroacher\n",
      "D:\\Species\\test_cropped\\Flat-headed cat\n",
      "D:\\Species\\test_cropped\\Ghost\n",
      "D:\\Species\\test_cropped\\Giant squirrel\n",
      "D:\\Species\\test_cropped\\Great argus\n",
      "D:\\Species\\test_cropped\\Greater coucal\n",
      "D:\\Species\\test_cropped\\Grey langur\n",
      "D:\\Species\\test_cropped\\Horse-tailed squirrel\n",
      "D:\\Species\\test_cropped\\Human\n",
      "D:\\Species\\test_cropped\\Leopard cat\n",
      "D:\\Species\\test_cropped\\Lizard\n",
      "D:\\Species\\test_cropped\\Long-tailed macaque\n",
      "D:\\Species\\test_cropped\\Long-tailed porcupine\n",
      "D:\\Species\\test_cropped\\Malay badger\n",
      "D:\\Species\\test_cropped\\Malay civet\n",
      "D:\\Species\\test_cropped\\Malaysian night heron\n",
      "D:\\Species\\test_cropped\\Marbled cat\n",
      "D:\\Species\\test_cropped\\Maroon langur\n",
      "D:\\Species\\test_cropped\\Mongoose\n",
      "D:\\Species\\test_cropped\\Monitor lizard\n",
      "D:\\Species\\test_cropped\\Moonrat\n",
      "D:\\Species\\test_cropped\\Mouse deer\n",
      "D:\\Species\\test_cropped\\Mousedeer\n",
      "D:\\Species\\test_cropped\\Orangutan\n",
      "D:\\Species\\test_cropped\\Oriental bay owl\n",
      "D:\\Species\\test_cropped\\Otter\n",
      "D:\\Species\\test_cropped\\Otter civet\n",
      "D:\\Species\\test_cropped\\Pangolin\n",
      "D:\\Species\\test_cropped\\Pig-tailed macaque\n",
      "D:\\Species\\test_cropped\\Poacher\n",
      "D:\\Species\\test_cropped\\Porcupine\n",
      "D:\\Species\\test_cropped\\Rat\n",
      "D:\\Species\\test_cropped\\Red junglefowl\n",
      "D:\\Species\\test_cropped\\Red munjac\n",
      "D:\\Species\\test_cropped\\Red muntjac\n",
      "D:\\Species\\test_cropped\\Roulroul\n",
      "D:\\Species\\test_cropped\\Sambar deer\n",
      "D:\\Species\\test_cropped\\SFD Staff\n",
      "D:\\Species\\test_cropped\\Short-tailed babbler\n",
      "D:\\Species\\test_cropped\\Short-tailed mongoose\n",
      "D:\\Species\\test_cropped\\Skink\n",
      "D:\\Species\\test_cropped\\Small-clawed otter\n",
      "D:\\Species\\test_cropped\\Smooth otter\n",
      "D:\\Species\\test_cropped\\Snail\n",
      "D:\\Species\\test_cropped\\Snake\n",
      "D:\\Species\\test_cropped\\Squirrel\n",
      "D:\\Species\\test_cropped\\Storm's stork\n",
      "D:\\Species\\test_cropped\\Sun bear\n",
      "D:\\Species\\test_cropped\\Team\n",
      "D:\\Species\\test_cropped\\Thick-spined porcupine\n",
      "D:\\Species\\test_cropped\\Tree shrew\n",
      "D:\\Species\\test_cropped\\Tufted ground squirrel\n",
      "D:\\Species\\test_cropped\\Unknown\n",
      "D:\\Species\\test_cropped\\Unsure\n",
      "D:\\Species\\test_cropped\\Vehicle\n",
      "D:\\Species\\test_cropped\\Western tarsier\n",
      "D:\\Species\\test_cropped\\White-crowned sharma\n",
      "D:\\Species\\test_cropped\\Yellow-bellied prinia\n",
      "D:\\Species\\test_cropped\\Yellow-throated marten\n"
     ]
    }
   ],
   "source": [
    "# 'U:\\\\PycharmProjects\\\\AnimalBiometrics\\\\data\\\\Clouded_leopard_ID\\\\train_cleaned_copy'\n",
    "# train_test_split(\"D:\\Species\\cropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(crop_top=0, crop_bottom=0, path='U:\\\\PycharmProjects\\\\AnimalBiometrics\\\\data\\\\Clouded_leopard_ID\\\\train_cleaned_copy'):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        print(dirs)\n",
    "        if dirs == []:\n",
    "            files = np.array(files)\n",
    "            subdir_new = str(subdir).replace('\\\\raw\\\\','\\\\train_cropped\\\\')\n",
    "            if not (os.path.exists(subdir_new) and os.path.isdir(subdir_new)):\n",
    "                os.makedirs(subdir_new)\n",
    "            for elem in files:\n",
    "                if elem == 'Thumbs.db': continue\n",
    "                img = Image.open(subdir + '\\\\' + elem)\n",
    "                width, height = img.size\n",
    "                cropped_img = img.crop((0, 0+crop_top, width, height-crop_bottom))\n",
    "#                 cropped_img.show()\n",
    "                cropped_img.save(subdir_new + '\\\\' + elem)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_images(30,65,\"D:/Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('U:/PycharmProjects/AnimalBiometrics/data/annotations/Training_boxes_animal_biometrics.csv', header=0, usecols=[\"Label\",'External ID','Dataset Name'])\n",
    "test_df = pd.read_csv('U:/PycharmProjects/AnimalBiometrics/data/annotations/Test_boxes_animal_biometrics.csv', header=0, usecols=[\"Label\",'External ID','Dataset Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "test_df_copy = test_df.copy()\n",
    "threshold = 4\n",
    "\n",
    "def convert_to_retina_net_csv(df):\n",
    "    train_out = []\n",
    "\n",
    "    for i, data_point in enumerate(df.values):\n",
    "        bbox, img_id, label = data_point\n",
    "        \n",
    "        im_path = 'U:/PycharmProjects/AnimalBiometrics/data/Clouded_leopard_ID/train_cleaned_cropped/{0}/'.format(label)\n",
    "        \n",
    "        if sum((len(list(filter(lambda a: a != 'Thumbs.db', f))) for _, _, f in os.walk(im_path))) < threshold: continue\n",
    "        \n",
    "        if bbox == 'Skip':\n",
    "            x_min, x_max, y_min, y_max = None, None, None, None\n",
    "        else:\n",
    "            bbox = json.loads(bbox)\n",
    "            coordinates = bbox['Animal'][0]['geometry']\n",
    "#             print(coordinates)\n",
    "            ys = [coordinates[i]['y'] for i in range(4)]\n",
    "            xs = [coordinates[i]['x'] for i in range(4)]\n",
    "\n",
    "        entry = [os.path.join(im_path, img_id), np.min(xs), np.min(ys), np.max(xs), np.max(ys), label]\n",
    "        train_out.append(entry)\n",
    "\n",
    "    bbox_df = pd.DataFrame(data=train_out, dtype=np.int32)\n",
    "    return bbox_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_retina_net_csv(train_df_copy).to_csv('./data/annotations/labeled_bboxes_train_RetinaNet_min_4.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_mapping(path='U:\\\\PycharmProjects\\\\AnimalBiometrics\\\\data\\\\Clouded_leopard_ID\\\\train_cleaned_copy'):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        if dirs == []: break\n",
    "        class_name_mapping_df = pd.DataFrame(list(zip(dirs,range(len(dirs)))))\n",
    "        class_name_mapping_df.columns = ['class_name','id']\n",
    "        class_name_mapping_df.to_csv('./data/annotations/class_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_class_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_members(zip):\n",
    "    parts = []\n",
    "    # get all the path prefixes\n",
    "    for name in zip.namelist():\n",
    "        # only check files (not directories)\n",
    "        if not name.endswith('/'):\n",
    "            # keep list of path elements (minus filename)\n",
    "            parts.append(name.split('/')[:-1])\n",
    "    # now find the common path prefix (if any)\n",
    "    prefix = os.path.commonprefix(parts)\n",
    "    if prefix:\n",
    "        # re-join the path elements\n",
    "        prefix = '/'.join(prefix) + '/'\n",
    "    # get the length of the common prefix\n",
    "    offset = len(prefix)\n",
    "    # now re-set the filenames\n",
    "    for zipinfo in zip.infolist():\n",
    "        name = zipinfo.filename\n",
    "        # only check files (not directories)\n",
    "        if len(name) > offset:\n",
    "            # remove the common prefix\n",
    "            zipinfo.filename = name[offset:]\n",
    "            yield zipinfo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/Species\"\n",
    "_zip = zipfile.ZipFile(os.path.join(path, 'sdc01.zip'),\"r\")\n",
    "_zip.extractall(path, get_members(_zip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_zips(path='D:/Species'):\n",
    "    zips = [name for name in os.listdir(path) if zipfile.is_zipfile(os.path.join(path, name)) ]\n",
    "\n",
    "    for file in zips:\n",
    "        print(file)\n",
    "        temp = zipfile.ZipFile(os.path.join(path, file),\"r\")\n",
    "        temp.extractall(path, get_members(temp))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdc01.zip\n",
      "sdc02.zip\n",
      "sdc03.zip\n",
      "sdc04.zip\n",
      "sdc05.zip\n",
      "sdc06.zip\n",
      "sdc07.zip\n",
      "sdc08.zip\n",
      "sdc09.zip\n",
      "sdc11.zip\n",
      "sdc12.zip\n",
      "sdc13.zip\n",
      "sdc14.zip\n",
      "sdc15.zip\n",
      "sdc16.zip\n",
      "sdc17.zip\n",
      "sdc18.zip\n",
      "sdc19.zip\n",
      "sdc20.zip\n",
      "sdc21.zip\n",
      "sdc22.zip\n",
      "sdc23.zip\n",
      "sdc24.zip\n",
      "sdc25.zip\n",
      "sdc26.zip\n",
      "sdc27.zip\n",
      "sdc28.zip\n",
      "sdc29.zip\n",
      "sdc30.zip\n",
      "sdc31.zip\n",
      "sdc32.zip\n",
      "sdc33.zip\n",
      "sdc34.zip\n",
      "sdc35.zip\n",
      "sdc36.zip\n",
      "sdc37.zip\n",
      "sdc38.zip\n",
      "sdc39.zip\n",
      "sdc40.zip\n",
      "sdc41.zip\n",
      "sdc42.zip\n",
      "sdc43.zip\n",
      "sdc44.zip\n",
      "sdc45.zip\n",
      "sdc46.zip\n",
      "sdc47.zip\n",
      "sdc48.zip\n",
      "sdc49.zip\n",
      "sdc50.zip\n",
      "sdc51.zip\n",
      "sdc52.zip\n",
      "sdc53.zip\n",
      "sdc54.zip\n",
      "sdc55.zip\n",
      "sdc56.zip\n",
      "sdc57.zip\n",
      "sdc58.zip\n",
      "sdc59.zip\n",
      "sdc60.zip\n",
      "sdc61.zip\n",
      "sdc62.zip\n",
      "sdc63.zip\n",
      "sdc64.zip\n",
      "SDF01.zip\n",
      "SDF02.zip\n",
      "SDF03.zip\n",
      "SDF04.zip\n",
      "SDF05.zip\n",
      "SDF06.zip\n",
      "SDF07.zip\n",
      "SDF08.zip\n",
      "SDF09.zip\n",
      "SDF10.zip\n",
      "SDF11.zip\n",
      "SDF12.zip\n",
      "SDF13.zip\n",
      "SDF14.zip\n",
      "SDF15.zip\n",
      "SDF16.zip\n",
      "SDF17.zip\n",
      "SDF18.zip\n",
      "SDF19.zip\n",
      "SDF20.zip\n",
      "SDF21.zip\n",
      "SDF22.zip\n",
      "SDF24.zip\n",
      "SDF25.zip\n",
      "SDF26.zip\n",
      "SDF27.zip\n",
      "SDF28.zip\n",
      "SDF29.zip\n",
      "SDF30.zip\n",
      "SDF31.zip\n",
      "SDF32.zip\n",
      "SDF33.zip\n",
      "SDF34.zip\n",
      "SDF35.zip\n",
      "SDF36.zip\n",
      "SDF37.zip\n",
      "SDF38.zip\n",
      "SDF39.zip\n",
      "SDF40.zip\n",
      "SDF41.zip\n",
      "SDF42.zip\n",
      "SDF43.zip\n",
      "SDF44.zip\n",
      "SDF45.zip\n",
      "SDF46.zip\n",
      "SDF47.zip\n",
      "SDF48.zip\n",
      "SDF49.zip\n",
      "SDF50.zip\n",
      "SDF51.zip\n",
      "SDF52.zip\n",
      "SDF53.zip\n",
      "SDF54.zip\n",
      "SDF55.zip\n",
      "SDF56.zip\n",
      "SDF57.zip\n",
      "SDF58.zip\n",
      "SDF59.zip\n",
      "SDF60.zip\n",
      "SDF61.zip\n",
      "SDF62.zip\n",
      "SDF63.zip\n",
      "SDF64.zip\n",
      "SKC01.zip\n",
      "SKC02.zip\n",
      "SKC03.zip\n",
      "SKC04.zip\n",
      "SKC06.zip\n",
      "SKC07.zip\n",
      "SKC08.zip\n",
      "SKC09.zip\n",
      "SKC10.zip\n",
      "SKC11.zip\n",
      "SKC12.zip\n",
      "SKC13.zip\n",
      "SKC15.zip\n",
      "SKC16.zip\n",
      "SKC17.zip\n",
      "SKC18.zip\n",
      "SKC19.zip\n",
      "SKC20.zip\n",
      "SKC21.zip\n",
      "SKC22.zip\n",
      "SKC23.zip\n",
      "SKC24.zip\n",
      "SKC25.zip\n",
      "SKC26.zip\n",
      "SKC27.zip\n",
      "SKC28.zip\n",
      "SKC29.zip\n",
      "SKC30.zip\n",
      "SKC31.zip\n",
      "SKC32.zip\n",
      "SKC33.zip\n",
      "SKC34.zip\n",
      "SKC35.zip\n",
      "SKC36.zip\n",
      "SKC37.zip\n",
      "SKC38.zip\n",
      "SKC39.zip\n",
      "SKC40.zip\n",
      "SKC41.zip\n",
      "SKC42.zip\n",
      "SKC43.zip\n",
      "SKC44.zip\n",
      "SKC45.zip\n",
      "SKC46.zip\n",
      "SKC47.zip\n",
      "SKC48.zip\n",
      "SKC50.zip\n",
      "SKC51.zip\n",
      "SKC52.zip\n",
      "SKC53.zip\n",
      "SKC54.zip\n",
      "SKC55.zip\n",
      "SKC62.zip\n",
      "STC01.zip\n",
      "STC02.zip\n",
      "STC03.zip\n",
      "STC04.zip\n",
      "STC05.zip\n",
      "STC06.zip\n",
      "STC07.zip\n",
      "STC08.zip\n",
      "STC09.zip\n",
      "STC10.zip\n",
      "STC11.zip\n",
      "STC12.zip\n",
      "STC13.zip\n",
      "STC14.zip\n",
      "STC15.zip\n",
      "STC16.zip\n",
      "STC17.zip\n",
      "STC18.zip\n",
      "STC19.zip\n",
      "STC20.zip\n",
      "STC21.zip\n",
      "STC22.zip\n",
      "STC23.zip\n",
      "STC24.zip\n",
      "STC25.zip\n",
      "STC26.zip\n",
      "STC27.zip\n",
      "STC28.zip\n",
      "STC29.zip\n",
      "STC30.zip\n",
      "STC31.zip\n",
      "STC32.zip\n",
      "STC33.zip\n",
      "STC34.zip\n",
      "STC36.zip\n",
      "STC37.zip\n",
      "STC38.zip\n",
      "STC39.zip\n",
      "STC40.zip\n",
      "STC41.zip\n",
      "STC42.zip\n",
      "STC43.zip\n",
      "STC44.zip\n",
      "STC45.zip\n",
      "STC46.zip\n",
      "STC47.zip\n",
      "STC49.zip\n",
      "STC50.zip\n",
      "STC51.zip\n",
      "STC52.zip\n",
      "STC53.zip\n",
      "STC54.zip\n",
      "STC55.zip\n",
      "STC56.zip\n",
      "STC57.zip\n",
      "STC58.zip\n",
      "STC59.zip\n",
      "STC60.zip\n",
      "STC61.zip\n",
      "STC62.zip\n",
      "STC63.zip\n",
      "STC64.zip\n",
      "STC65.zip\n",
      "STC66.zip\n",
      "stf01.zip\n",
      "stf02.zip\n",
      "stf03.zip\n",
      "stf04.zip\n",
      "stf05.zip\n",
      "stf06.zip\n",
      "stf07.zip\n",
      "stf08.zip\n",
      "stf09.zip\n",
      "stf10.zip\n",
      "stf11.zip\n",
      "stf12.zip\n",
      "stf13.zip\n",
      "stf14.zip\n",
      "stf15.zip\n",
      "stf16.zip\n",
      "stf17.zip\n",
      "stf18.zip\n",
      "stf19.zip\n",
      "stf20.zip\n",
      "stf21.zip\n",
      "stf22.zip\n",
      "stf23.zip\n",
      "stf24.zip\n",
      "stf25.zip\n",
      "stf26.zip\n",
      "stf27.zip\n",
      "stf28.zip\n",
      "stf29.zip\n",
      "stf30.zip\n",
      "stf31.zip\n",
      "stf32.zip\n",
      "stf33.zip\n",
      "stf34.zip\n",
      "stf35.zip\n",
      "stf36.zip\n",
      "stf37.zip\n",
      "stf38.zip\n",
      "stf39.zip\n",
      "stf40.zip\n",
      "stf41.zip\n",
      "stf42.zip\n",
      "stf43.zip\n",
      "stf44.zip\n",
      "stf45.zip\n",
      "stf46.zip\n",
      "stf47.zip\n",
      "stf48.zip\n",
      "stf49.zip\n",
      "stf50.zip\n",
      "stf51.zip\n",
      "stf52.zip\n",
      "stf53.zip\n",
      "stf54.zip\n",
      "stf55.zip\n",
      "stf56.zip\n",
      "stf57.zip\n",
      "stf58.zip\n",
      "stf59.zip\n",
      "stf60.zip\n",
      "stf61.zip\n",
      "stf62.zip\n",
      "stf63.zip\n",
      "stf64.zip\n"
     ]
    }
   ],
   "source": [
    "extract_zips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images_root(crop_top=0, crop_bottom=0, path='U:\\\\PycharmProjects\\\\AnimalBiometrics\\\\data\\\\Clouded_leopard_ID\\\\train_cleaned_copy'):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        print(subdir, 'cropped' not in subdir)\n",
    "        if dirs == [] and 'cropped' not in subdir:\n",
    "            files = np.array(files)\n",
    "            split = str(subdir).split(\"\\\\\")\n",
    "            subdir_new = os.path.join(split[0],\"cropped\",split[1])\n",
    "            if not (os.path.exists(subdir_new) and os.path.isdir(subdir_new)):\n",
    "                os.makedirs(subdir_new)\n",
    "            for elem in files:\n",
    "                img_path_new = subdir_new + '\\\\' + elem\n",
    "#                 print(os.path.exists(img_path_new))\n",
    "                if elem == 'Thumbs.db' or os.path.exists(img_path_new): continue\n",
    "                try:\n",
    "                    img = Image.open(subdir + '\\\\' + elem)\n",
    "                    width, height = img.size\n",
    "                    cropped_img = img.crop((0, 0+crop_top, width, height-crop_bottom))\n",
    "    #                 cropped_img.show()\n",
    "                    cropped_img.save(subdir_new + '\\\\' + elem)\n",
    "                except: \n",
    "                    print(subdir + '\\\\' + elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Species True\n",
      "D:/Species\\Asian small-clawed otter True\n",
      "D:/Species\\Banded civet True\n",
      "D:/Species\\Banded linsang True\n",
      "D:/Species\\Banteng True\n",
      "D:/Species\\Bay cat True\n",
      "D:/Species\\Bearded pig True\n",
      "D:/Species\\Bearded pig\\Sambar deer True\n",
      "D:/Species\\Binturong True\n",
      "D:/Species\\Bird True\n",
      "D:/Species\\Black-capped babbler True\n",
      "D:/Species\\Black-crowned pitta True\n",
      "D:/Species\\Blue flycather True\n",
      "D:/Species\\Blue-headed pitta True\n",
      "D:/Species\\Bornean ground-cuckoo True\n",
      "D:/Species\\Bornean necklaced partridge True\n",
      "D:/Species\\Bornean peacock pheasant True\n",
      "D:/Species\\Bornean yellow muntjac True\n",
      "D:/Species\\Buff-vented bulbul True\n",
      "D:/Species\\Bushy fish owl True\n",
      "D:/Species\\Changeable hawk eagle True\n",
      "D:/Species\\Clouded leopard True\n",
      "D:/Species\\Collared mongoose True\n",
      "D:/Species\\Common palm civet True\n",
      "D:/Species\\Common porcupine True\n",
      "D:/Species\\Crested fireback True\n",
      "D:/Species\\Crested serpent eagle True\n",
      "D:/Species\\cropped False\n",
      "D:/Species\\cropped\\Asian small-clawed otter False\n",
      "D:/Species\\cropped\\Banded civet False\n",
      "D:/Species\\cropped\\Banded linsang False\n",
      "D:/Species\\cropped\\Banteng False\n",
      "D:/Species\\cropped\\Bay cat False\n",
      "D:/Species\\cropped\\Bearded pig False\n",
      "D:/Species\\cropped\\Binturong False\n",
      "D:/Species\\cropped\\Bird False\n",
      "D:/Species\\cropped\\Black-capped babbler False\n",
      "D:/Species\\cropped\\Black-crowned pitta False\n",
      "D:/Species\\cropped\\Blue flycather False\n",
      "D:/Species\\cropped\\Blue-headed pitta False\n",
      "D:/Species\\cropped\\Bornean ground-cuckoo False\n",
      "D:/Species\\cropped\\Bornean necklaced partridge False\n",
      "D:/Species\\cropped\\Bornean peacock pheasant False\n",
      "D:/Species\\cropped\\Bornean yellow muntjac False\n",
      "D:/Species\\cropped\\Buff-vented bulbul False\n",
      "D:/Species\\cropped\\Bushy fish owl False\n",
      "D:/Species\\cropped\\Changeable hawk eagle False\n",
      "D:/Species\\cropped\\Clouded leopard False\n",
      "D:/Species\\cropped\\Collared mongoose False\n",
      "D:/Species\\cropped\\Common palm civet False\n",
      "D:/Species\\cropped\\Common porcupine False\n",
      "D:/Species\\cropped\\Crested fireback False\n",
      "D:/Species\\cropped\\Crested serpent eagle False\n",
      "D:/Species\\cropped\\Dog False\n",
      "D:/Species\\cropped\\Elephant False\n",
      "D:/Species\\cropped\\Emerald dove False\n",
      "D:/Species\\cropped\\Encroacher False\n",
      "D:/Species\\cropped\\Flat-headed cat False\n",
      "D:/Species\\cropped\\Ghost False\n",
      "D:/Species\\Dog True\n",
      "D:/Species\\Elephant True\n",
      "D:/Species\\Emerald dove True\n",
      "D:/Species\\Encroacher True\n",
      "D:/Species\\Flat-headed cat True\n",
      "D:/Species\\Ghost True\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10001).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10002).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10003).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10004).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10005).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10006).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10007).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10008).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10009).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10010).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10011).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10012).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10013).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10014).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10015).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10016).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10017).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10018).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10019).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10020).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10021).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10022).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10023).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10024).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10025).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10026).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10027).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10028).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10029).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10030).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10031).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10032).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10033).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10034).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10035).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10036).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10037).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10038).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10039).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10040).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10041).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10042).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10043).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10044).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10045).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10046).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10047).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10048).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10049).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10050).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10051).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03(10052).JPG\n",
      "D:/Species\\Ghost\\SDC49__pc 076__2015-01-02__09-05-03.JPG\n",
      "D:/Species\\Giant squirrel True\n",
      "D:/Species\\Great argus True\n",
      "D:/Species\\Greater coucal True\n",
      "D:/Species\\Grey langur True\n",
      "D:/Species\\Horse-tailed squirrel True\n",
      "D:/Species\\Human True\n",
      "D:/Species\\Leopard cat True\n",
      "D:/Species\\Lizard True\n",
      "D:/Species\\Long-tailed macaque True\n",
      "D:/Species\\Long-tailed porcupine True\n",
      "D:/Species\\Malay badger True\n",
      "D:/Species\\Malay civet True\n",
      "D:/Species\\Malaysian night heron True\n",
      "D:/Species\\Marbled cat True\n",
      "D:/Species\\Maroon langur True\n",
      "D:/Species\\Mongoose True\n",
      "D:/Species\\Monitor lizard True\n",
      "D:/Species\\Moonrat True\n",
      "D:/Species\\Mouse deer True\n",
      "D:/Species\\Mousedeer True\n",
      "D:/Species\\Orangutan True\n",
      "D:/Species\\Oriental bay owl True\n",
      "D:/Species\\Otter True\n",
      "D:/Species\\Otter civet True\n",
      "D:/Species\\Pangolin True\n",
      "D:/Species\\Pig-tailed macaque True\n",
      "D:/Species\\Poacher True\n",
      "D:/Species\\Porcupine True\n",
      "D:/Species\\Rat True\n",
      "D:/Species\\Red junglefowl True\n",
      "D:/Species\\Red munjac True\n",
      "D:/Species\\Red muntjac True\n",
      "D:/Species\\Roulroul True\n",
      "D:/Species\\Sambar deer True\n",
      "D:/Species\\SFD Staff True\n",
      "D:/Species\\Short-tailed babbler True\n",
      "D:/Species\\Short-tailed mongoose True\n",
      "D:/Species\\Skink True\n",
      "D:/Species\\Small-clawed otter True\n",
      "D:/Species\\Smooth otter True\n",
      "D:/Species\\Snail True\n",
      "D:/Species\\Snake True\n",
      "D:/Species\\Squirrel True\n",
      "D:/Species\\Storm's stork True\n",
      "D:/Species\\Sun bear True\n",
      "D:/Species\\Team True\n",
      "D:/Species\\Thick-spined porcupine True\n",
      "D:/Species\\Tree shrew True\n",
      "D:/Species\\Tufted ground squirrel True\n",
      "D:/Species\\Unknown True\n",
      "D:/Species\\Unsure True\n",
      "D:/Species\\Vehicle True\n",
      "D:/Species\\Western tarsier True\n",
      "D:/Species\\White-crowned sharma True\n",
      "D:/Species\\Yellow-bellied prinia True\n",
      "D:/Species\\Yellow-throated marten True\n"
     ]
    }
   ],
   "source": [
    "crop_images_root(30,65,\"D:/Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_activation\n",
    "\n",
    "def print_comparison(indices, images, predictions, labels, trained_model, activation_layer='predictions'):\n",
    "    \n",
    "    trained_model.layers[utils.find_layer_idx(trained_model, \"predictions\")].activation = keras.activations.linear\n",
    "    \n",
    "    for i, img_number in enumerate(indices):\n",
    "        for j in range(50, 98):\n",
    "            activation_layer='activation_{}'.format(j)\n",
    "            fig = plt.figure(i, figsize=(20,10))\n",
    "            ax1 = fig.add_subplot(121)\n",
    "    #         print(images)\n",
    "            ax1.set_title(class_dict[labels[img_number]])\n",
    "            ax1.imshow(images[img_number])\n",
    "            activation = visualize_activation(n_model, utils.find_layer_idx(trained_model, activation_layer),\n",
    "                                       seed_input=images[img_number],\n",
    "                                       filter_indices=[labels[img_number]],\n",
    "    #                                    filter_indices=None,\n",
    "    #                                        penultimate_layer_idx=utils.find_layer_idx(trained_model, activation_layer),\n",
    "    #                                    penultimate_layer_idx=None,\n",
    "                                       backprop_modifier=None, \n",
    "                                       grad_modifier=None)\n",
    "\n",
    "            ax2 = fig.add_subplot(122)\n",
    "            ax2.set_title(class_dict[predictions[img_number]])\n",
    "            ax2.imshow(activation)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_comparison(wrong_pred, data_array, prediction_heat, data_array_label, n_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_comparison(correct_pred, data_array, prediction_heat, data_array_label, n_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
